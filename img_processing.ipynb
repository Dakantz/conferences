{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import tqdm\n",
    "import Quartz\n",
    "import Vision\n",
    "import CoreFoundation\n",
    "from Cocoa import NSURL\n",
    "from Foundation import NSDictionary, NSArray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# needed to capture system-level stderr\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "    \"\"\"Rearrange coordinates to order:\n",
    "    top-left, top-right, bottom-right, bottom-left\"\"\"\n",
    "    rect = np.zeros((4, 2), dtype=\"float32\")\n",
    "    pts = np.array(pts)\n",
    "    s = pts.sum(axis=1)\n",
    "    # Top-left point will have the smallest sum.\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    # Bottom-right point will have the largest sum.\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    diff = np.diff(pts, axis=1)\n",
    "    # Top-right point will have the smallest difference.\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    # Bottom-left will have the largest difference.\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    # return the ordered coordinates\n",
    "    return rect.astype(\"int\").tolist()\n",
    "\n",
    "\n",
    "def find_dest(pts):\n",
    "    (tl, tr, br, bl) = pts\n",
    "    # Finding the maximum width.\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "\n",
    "    # Finding the maximum height.\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "    # Final destination co-ordinates.\n",
    "    destination_corners = [[0, 0], [maxWidth, 0], [maxWidth, maxHeight], [0, maxHeight]]\n",
    "\n",
    "    return order_points(destination_corners)\n",
    "\n",
    "\n",
    "def scan(img, name=\"\"):\n",
    "    # Resize image to workable size\n",
    "    dim_limit = 1920\n",
    "    max_dim = max(img.shape)\n",
    "    if max_dim > dim_limit:\n",
    "        resize_scale = dim_limit / max_dim\n",
    "        img = cv2.resize(img, None, fx=resize_scale, fy=resize_scale)\n",
    "    # Create a copy of resized original image for later use\n",
    "    orig_img = img.copy()\n",
    "    # Repeated Closing operation to remove text from the document.\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iterations=3)\n",
    "    # GrabCut\n",
    "    mask = np.zeros(img.shape[:2], np.uint8)\n",
    "    bgdModel = np.zeros((1, 65), np.float64)\n",
    "    fgdModel = np.zeros((1, 65), np.float64)\n",
    "    rect = (20, 20, img.shape[1] - 20, img.shape[0] - 20)\n",
    "    cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "    mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype(\"uint8\")\n",
    "    img = img * mask2[:, :, np.newaxis]\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (11, 11), 0)\n",
    "    # Edge Detection.\n",
    "    canny = cv2.Canny(gray, 0, 200)\n",
    "    canny = cv2.dilate(canny, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)))\n",
    "\n",
    "    # Finding contours for the detected edges.\n",
    "    contours, hierarchy = cv2.findContours(canny, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    # Keeping only the largest detected contour.\n",
    "    page = sorted(contours, key=cv2.contourArea, reverse=True)[:5]\n",
    "\n",
    "    # Detecting Edges through Contour approximation.\n",
    "    # Loop over the contours.\n",
    "    if len(page) == 0:\n",
    "        return orig_img\n",
    "    candidates = []\n",
    "    for c in page:\n",
    "        # Approximate the contour.\n",
    "        epsilon = 0.02 * cv2.arcLength(c, True)\n",
    "        corners = cv2.approxPolyDP(c, epsilon, True)\n",
    "        area = cv2.contourArea(corners)\n",
    "        # If our approximated contour has four points.\n",
    "        if len(corners) == 4:\n",
    "            candidates.append((corners, area))\n",
    "    if len(candidates) == 0:\n",
    "        return orig_img\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    corners = candidates[0][0]\n",
    "    print(candidates)\n",
    "    # Sorting the corners and converting them to desired shape.\n",
    "    corners = sorted(np.concatenate(corners).tolist())\n",
    "    # For 4 corner points being detected.\n",
    "    corners = order_points(corners)\n",
    "\n",
    "    destination_corners = find_dest(corners)\n",
    "\n",
    "    h, w = orig_img.shape[:2]\n",
    "    # Getting the homography.\n",
    "    M = cv2.getPerspectiveTransform(\n",
    "        np.float32(corners), np.float32(destination_corners)\n",
    "    )\n",
    "    # Perspective transform using homography.\n",
    "    final = cv2.warpPerspective(\n",
    "        orig_img,\n",
    "        M,\n",
    "        (destination_corners[2][0], destination_corners[2][1]),\n",
    "        flags=cv2.INTER_LINEAR,\n",
    "    )\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = glob(\"pics/*.jpeg\")\n",
    "# for img in tqdm(imgs):\n",
    "#     print(img)\n",
    "#     img_dat=cv2.imread(img)\n",
    "#     final=scan(img_dat)\n",
    "#     cv2.imwrite('out/'+img.split('/')[-1], final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request_handler_img(results):\n",
    "    \"\"\"results: list to store results\"\"\"\n",
    "    if not isinstance(results, list):\n",
    "        raise ValueError(\"results must be a list\")\n",
    "\n",
    "    def handler(request, error):\n",
    "        if error:\n",
    "            print(f\"Error! {error}\")\n",
    "        else:\n",
    "            observations: \"list[Vision.VNRecognizedTextObservation]\" = request.results()\n",
    "            for text_observation in observations:\n",
    "                recognized_text = text_observation.topCandidates_(1)[0]\n",
    "                corners = {\n",
    "                    \"tl\": text_observation.topLeft(),\n",
    "                    \"tr\": text_observation.topRight(),\n",
    "                    \"bl\": text_observation.bottomLeft(),\n",
    "                    \"br\": text_observation.bottomRight(),\n",
    "                }\n",
    "                corners = {k: (v.x, v.y) for k, v in corners.items()}\n",
    "                results.append(\n",
    "                    [recognized_text.string(), recognized_text.confidence(), corners]\n",
    "                )\n",
    "\n",
    "    return handler\n",
    "\n",
    "\n",
    "def image_to_text(\n",
    "    img_path, lang=\"eng\"\n",
    ") -> \"list[tuple[str, float, dict[str,tuple[float,float]]]]\":\n",
    "    input_url = NSURL.fileURLWithPath_(img_path)\n",
    "\n",
    "    input_image = Quartz.CIImage.imageWithContentsOfURL_(input_url)\n",
    "\n",
    "    vision_options = NSDictionary.dictionaryWithDictionary_({})\n",
    "\n",
    "    vision_handler = Vision.VNImageRequestHandler.alloc().initWithCIImage_options_(\n",
    "        input_image, vision_options\n",
    "    )\n",
    "    results = []\n",
    "    handler = make_request_handler_img(results)\n",
    "    vision_request = Vision.VNRecognizeTextRequest.alloc().initWithCompletionHandler_(\n",
    "        handler\n",
    "    )\n",
    "    # print(vision_request.recognitionLanguages())\n",
    "    vision_request.setRecognitionLanguages_(\n",
    "        NSArray.arrayWithArray_(\n",
    "            [\n",
    "                lang,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    # vision_request.setCustomWords_(NSArray.arrayWithArray_(['f√ºr',]))\n",
    "    # print(type(vision_request.recognitionLanguages()))\n",
    "    # print(vision_request.recognitionLanguages())\n",
    "    vision_request.setUsesCPUOnly_(False)  # somehow improves accuracy??\n",
    "    error = vision_handler.performRequests_error_([vision_request], None)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dest_doc(pts):\n",
    "    (tl, tr, br, bl) = pts\n",
    "    # Finding the maximum width.\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "\n",
    "    # Finding the maximum height.\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "    # Final destination co-ordinates.\n",
    "    destination_corners = [[0, maxHeight], [maxWidth, maxHeight], [0, 0], [maxWidth, 0]]\n",
    "\n",
    "    return destination_corners\n",
    "\n",
    "\n",
    "def make_request_handler_doc(results):\n",
    "    \"\"\"results: list to store results\"\"\"\n",
    "    if not isinstance(results, list):\n",
    "        raise ValueError(\"results must be a list\")\n",
    "\n",
    "    def handler(request, error):\n",
    "        if error:\n",
    "            print(f\"Error! {error}\")\n",
    "        else:\n",
    "            observations = request.results()\n",
    "            for obs in observations:\n",
    "                # print(obs)\n",
    "                # print(obs.bottomLeft())\n",
    "                # print(obs.bottomRight())\n",
    "                bbox = obs.boundingBox()\n",
    "                corners = {\n",
    "                    \"tl\": obs.topLeft(),\n",
    "                    \"tr\": obs.topRight(),\n",
    "                    \"bl\": obs.bottomLeft(),\n",
    "                    \"br\": obs.bottomRight(),\n",
    "                }\n",
    "                corners = {k: (v.x, v.y) for k, v in corners.items()}\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"bbox\": corners,\n",
    "                        \"bbox_obj\": bbox,\n",
    "                        \"conf\": obs.confidence(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return handler\n",
    "\n",
    "\n",
    "def image_doc_handler(img_path: str) -> str:\n",
    "    input_url = NSURL.fileURLWithPath_(img_path)\n",
    "\n",
    "    input_image = Quartz.CIImage.imageWithContentsOfURL_(input_url)\n",
    "\n",
    "    vision_options = NSDictionary.dictionaryWithDictionary_({})\n",
    "\n",
    "    vision_handler = Vision.VNImageRequestHandler.alloc().initWithCIImage_options_(\n",
    "        input_image, vision_options\n",
    "    )\n",
    "    results = []\n",
    "    handler = make_request_handler_doc(results)\n",
    "    vision_request = (\n",
    "        Vision.VNDetectDocumentSegmentationRequest.alloc().initWithCompletionHandler_(\n",
    "            handler\n",
    "        )\n",
    "    )\n",
    "    # print(vision_request.recognitionLanguages())\n",
    "    # vision_request.setRecognitionLanguages_(NSArray.arrayWithArray_([lang,]))\n",
    "    # vision_request.setCustomWords_(NSArray.arrayWithArray_(['f√ºr',]))\n",
    "    # print(type(vision_request.recognitionLanguages()))\n",
    "    # print(vision_request.recognitionLanguages())\n",
    "    # vision_request.setUsesCPUOnly_(False) # somehow improves accuracy??\n",
    "    error = vision_handler.performRequests_error_([vision_request], None)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "res = image_doc_handler(imgs[0])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imgs[4]\n",
    "res = image_doc_handler(img)\n",
    "img_dat = cv2.imread(img)\n",
    "src = np.array([list(p) for p in res[0][\"bbox\"].values()])\n",
    "dst = np.array([[0, 1], [1, 1], [0, 0], [1, 0]])\n",
    "src = src * img_dat.shape[:-1]\n",
    "dst = find_dest_doc(dst * img_dat.shape[:-1])\n",
    "\n",
    "M = cv2.getPerspectiveTransform(src.astype(\"float32\"), np.array(dst).astype(\"float32\"))\n",
    "warped = cv2.warpPerspective(img_dat, M, img_dat.shape[:-1])\n",
    "\n",
    "\n",
    "src, dst\n",
    "\n",
    "plt.imshow(warped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authors_text(texts):\n",
    "    results = texts\n",
    "    for textbox in results:\n",
    "        commas = (\n",
    "            textbox[0].count(\",\")\n",
    "            + textbox[0].count(\"and\")\n",
    "            + textbox[0].count(\"&\")\n",
    "            + textbox[0].count(\".\")\n",
    "        )\n",
    "\n",
    "        textbox.append(commas)\n",
    "    results.sort(key=lambda x: x[-1], reverse=True)\n",
    "    return results[0][0] if len(results) else \"\"\n",
    "\n",
    "\n",
    "def biggest_text(texts):\n",
    "    results = texts\n",
    "    for textbox in results:\n",
    "        dims = textbox[2]\n",
    "        poss = np.array([dims[\"tl\"], dims[\"tr\"], dims[\"br\"], dims[\"bl\"]]).astype(\n",
    "            np.float32\n",
    "        )\n",
    "        area = cv2.contourArea(poss)\n",
    "        textbox.append(area)\n",
    "    results.sort(key=lambda x: x[-1], reverse=True)\n",
    "    return results[0][0] if len(results) else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [00:17<00:00,  3.19it/s]\n"
     ]
    }
   ],
   "source": [
    "outs = glob(\"out_good/*.jpeg\")\n",
    "for img in tqdm(outs):\n",
    "    img_dat = cv2.imread(img)\n",
    "    # downscale image\n",
    "    dim_limit = 1080\n",
    "    max_dim = max(img_dat.shape)\n",
    "    if max_dim > dim_limit:\n",
    "        resize_scale = dim_limit / max_dim\n",
    "        img_dat = cv2.resize(img_dat, None, fx=resize_scale, fy=resize_scale)\n",
    "    cv2.imwrite(\"out_reduced/\" + img.split(\"/\")[-1], img_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52/52 [12:13<00:00, 14.11s/it]  \n"
     ]
    }
   ],
   "source": [
    "outs = glob(\"out_good/*.jpeg\")\n",
    "data = []\n",
    "for img in tqdm(outs):\n",
    "\n",
    "    texts = image_to_text(img, \"eng\")\n",
    "    title = biggest_text(texts)\n",
    "    authors = authors_text(texts)\n",
    "\n",
    "    # print(title, authors)\n",
    "    data.append({\"title\": title, \"authors\": authors, \"texts\": texts, \"img\": img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52/52 [04:01<00:00,  4.65s/it]\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client()\n",
    "arxiv_data = []\n",
    "for poster in tqdm(data):\n",
    "    # poster = data[0]\n",
    "    query = f\"{poster['title'].lower()} {poster['authors'].lower().split(',')[0]}\"\n",
    "    search = arxiv.Search(\n",
    "        query=query, max_results=1, sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    results = list(client.results(search))\n",
    "    if len(results) > 0:\n",
    "        result = results[0]\n",
    "        poster[\"arxiv\"] = result\n",
    "        poster[\"title_a\"] = result.title\n",
    "        poster[\"authors_a\"] = \",\".join([a.name for a in result.authors])\n",
    "        poster[\"abstract_a\"] = result.summary\n",
    "        poster[\"url\"] = result.links[0].href\n",
    "    else:\n",
    "        poster[\"title_a\"] = poster[\"title\"]\n",
    "        poster[\"authors_a\"] = \",\".join([str(poster[\"authors\"])])\n",
    "        poster[\"abstract_a\"] = \"\"\n",
    "        poster[\"url\"] = \"\"\n",
    "\n",
    "    arxiv_data.append(poster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylatex.utils import escape_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./posters.tex\", \"w\") as f:\n",
    "    f.write(\"\\\\begin{longtblr}{|m{70mm}|X|}  \\n\")\n",
    "    f.write(\"\\\\hline \\n\")\n",
    "    f.write(\"Poster & Information  \\\\\\\\  \\n\")\n",
    "    f.write(\"\\\\hline \\n\")\n",
    "    for poster in arxiv_data:\n",
    "        f.write(\n",
    "            \"\\\\raisebox{-\\\\height}{\\s\\includegraphics[width=68mm]{\"\n",
    "            + \"out_reduced/\"\n",
    "            + poster[\"img\"].split(\"/\")[-1]\n",
    "            + \"}} & \\\\textbf{\"\n",
    "            + escape_latex(poster[\"title_a\"].replace(\"‚â•\", \" \"))\n",
    "            + \"} \\n \\\\textit{\"\n",
    "            + escape_latex(poster[\"authors_a\"].replace(\"‚â•\", \" \"))\n",
    "            + \"} \\n\\n\"\n",
    "            + escape_latex(poster[\"title\"].replace(\"‚â•\", \" \"))+ \"\\n\\n\"\n",
    "            + \"\\\\url{\"+escape_latex(poster[\"url\"])+\"}\"\n",
    "            + \"\\\\\\\\\"\n",
    "        )\n",
    "\n",
    "    f.write(\"\\\\end{longtblr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "everything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
